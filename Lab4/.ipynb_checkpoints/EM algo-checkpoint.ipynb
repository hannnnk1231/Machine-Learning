{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx3_decode(file):\n",
    "    data=open(file, 'rb').read()\n",
    "    # > for Big-endian, iiii for 4 integers, each size=4\n",
    "    fmt='>iiii'\n",
    "    offset=0\n",
    "    magic_number, image_numbers, height, width=struct.unpack_from(fmt,data,offset)\n",
    "    image_size=height*width\n",
    "    offset+=struct.calcsize(fmt)\n",
    "    # B for unsigned byte, size=1\n",
    "    fmt='>'+str(image_size)+'B'\n",
    "    images=np.empty((image_numbers,height*width))\n",
    "    for i in range(image_numbers):\n",
    "        images[i]=np.array(struct.unpack_from(fmt,data,offset)).reshape((height*width))\n",
    "        offset+=struct.calcsize(fmt)\n",
    "    return images,image_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx1_decode(file):\n",
    "    data=open(file, 'rb').read()\n",
    "    # > for Big-endian, ii for 2 integers, each size=4\n",
    "    fmt='>ii'\n",
    "    offset=0\n",
    "    magic_number, label_numbers=struct.unpack_from(fmt,data,offset)\n",
    "    offset+=struct.calcsize(fmt)\n",
    "    # B for unsigned byte, size=1\n",
    "    fmt='>B'\n",
    "    labels=np.empty(label_numbers)\n",
    "    for i in range(label_numbers):\n",
    "        labels[i]=struct.unpack_from(fmt,data,offset)[0]\n",
    "        offset+=struct.calcsize(fmt)\n",
    "    return labels,label_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step():\n",
    "    for n in range(60000):\n",
    "        temp = lamda.copy()\n",
    "        for k in range(10):\n",
    "            for d in range(784):\n",
    "                if (X[n,d]==1):\n",
    "                    if(P[d,k]==0):\n",
    "                        temp[k] *= 0.0001\n",
    "                    else: \n",
    "                        temp[k] *= P[d,k]\n",
    "                else:\n",
    "                    if(P[d,k]==1):\n",
    "                        temp[k] *= 0.0001\n",
    "                    else:\n",
    "                        temp[k] *= 1-P[d,k]\n",
    "        for k in range(10):\n",
    "            if(np.sum(temp)==0):\n",
    "                W[n,k] = temp[k]/0.0001\n",
    "            else:\n",
    "                W[n,k] = temp[k]/np.sum(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step():\n",
    "    sigma_w = np.sum(W,axis=0)\n",
    "    lamda = sigma_w/60000\n",
    "    for k in range(10):\n",
    "        for d in range(784):\n",
    "            P[d][k] = np.dot(np.transpose(X)[d],np.transpose(W)[k])\n",
    "            if(sigma_w[k]==0):\n",
    "                P[d][k] /= 0.0001\n",
    "            else:\n",
    "                P[d][k] /= sigma_w[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_imagination():\n",
    "    for k in range(10):\n",
    "        print('\\nclass {}:'.format(k))\n",
    "        for d in range(784):\n",
    "            if d%28==0 and d!=0:\n",
    "                print('')\n",
    "            if P[d,k]>0.5:\n",
    "                print('1',end='')\n",
    "            else:\n",
    "                print('0',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_labeled_imagination(r):\n",
    "    for i,k in enumerate(r):\n",
    "        print('labeled class {}:'.format(i))\n",
    "        for d in range(784):\n",
    "            if d%28==0 and d!=0:\n",
    "                print('')\n",
    "            if P[d,int(k)]>0.5:\n",
    "                print('1',end='')\n",
    "            else:\n",
    "                print('0',end='')\n",
    "        print('\\n')\n",
    "    print(\"\\n----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(r):\n",
    "    confusion_matrix = np.zeros((10,3))\n",
    "    error = 0\n",
    "    for n in range(60000):\n",
    "        temp = lamda.copy()\n",
    "        for k in range(10):\n",
    "            for d in range(784):\n",
    "                if (X[n,d]==1):\n",
    "                    if(P[d,k]==0):\n",
    "                        temp[k] *= 0.0001\n",
    "                    else: \n",
    "                        temp[k] *= P[d,k]\n",
    "                else:\n",
    "                    if(P[d,k]==1):\n",
    "                        temp[k] *= 0.0001\n",
    "                    else:\n",
    "                        temp[k] *= 1-P[d,k]\n",
    "        temp_index, = np.where(r==np.argmax(temp))[0]\n",
    "        if(int(train_label[n])==temp_index):\n",
    "            confusion_matrix[int(train_label[n]),0]+=1\n",
    "        else:\n",
    "            confusion_matrix[int(train_label[n]),1]+=1\n",
    "            confusion_matrix[temp_index,2]+=1\n",
    "    for k in range(10):\n",
    "        print(\"\\nConfusion Matrix:\") \n",
    "        print(\"                Predict number {} Predict not number {}\".format(k,k))\n",
    "        print(\"Is number {}           {}                {}\".format(k,confusion_matrix[k,0],confusion_matrix[k,1]))\n",
    "        print(\"Isn\\'t number {}       {}                {}\".format(k,confusion_matrix[k,2],60000-np.sum(confusion_matrix[k])))\n",
    "        print(\"\\nSensitivity (Successfully predict number {}): {:.5f}\".format(k,confusion_matrix[k,0]/(confusion_matrix[k,0]+confusion_matrix[k,1])))\n",
    "        print(\"Specificity (Successfully predict not number {}): {:.5f}\".format(k,(60000-np.sum(confusion_matrix[k]))/(confusion_matrix[k,2]+60000-np.sum(confusion_matrix[k]))))\n",
    "        print(\"\\n----------------------------------------------------\")\n",
    "        error+=confusion_matrix[k,1]+confusion_matrix[k,2]\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering():\n",
    "    table = np.zeros((10,10))\n",
    "    label_class_relation = np.zeros(10)\n",
    "    for n in range(60000):\n",
    "        temp = lamda.copy()\n",
    "        for k in range(10):\n",
    "            for d in range(784):\n",
    "                if (X[n,d]==1):\n",
    "                    if(P[d,k]==0):\n",
    "                        temp[k] *= 0.0001\n",
    "                    else: \n",
    "                        temp[k] *= P[d,k]\n",
    "                else:\n",
    "                    if(P[d,k]==1):\n",
    "                        temp[k] *= 0.0001\n",
    "                    else:\n",
    "                        temp[k] *= 1-P[d,k]\n",
    "        table[int(train_label[n]),np.argmax(temp)]+=1\n",
    "    print(table)\n",
    "    for k in range(10):\n",
    "        index = np.unravel_index(np.argmax(table, axis=None), table.shape)\n",
    "        label_class_relation[index[0]] = index[1]\n",
    "        for j in range(0, 10):\n",
    "            table[index[0]][j] = -1\n",
    "            table[j][index[1]] = -1\n",
    "    print(label_class_relation)\n",
    "    print_labeled_imagination(label_class_relation)\n",
    "    return confusion(label_class_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path='train-images.idx3-ubyte'\n",
    "train_label_path='train-labels.idx1-ubyte'\n",
    "train_image,train_image_number=idx3_decode(train_image_path)\n",
    "train_label,train_label_number=idx1_decode(train_label_path)\n",
    "train_image = train_image//128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_image.copy()\n",
    "lamda = np.full((10,1),0.1,dtype=np.float64) # init prob for every class\n",
    "P = np.random.rand(28*28,10) # init prob for every pixel of every class\n",
    "P_prev = P.copy()\n",
    "W = np.zeros((60000,10)) # init w for every pic for every class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-88d12a6eeb0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mE_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mM_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint_imagination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-e62e1aa35fcd>\u001b[0m in \u001b[0;36mE_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                         \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "while(1):\n",
    "    iteration += 1\n",
    "    E_step()\n",
    "    M_step()\n",
    "    print_imagination()\n",
    "    diff = np.sum(abs(P-P_prev))\n",
    "    print(\"\\nNo. of Iteration: {}, Difference: {}\".format(iteration, diff))\n",
    "    print(\"\\n----------------------------------------------------\")\n",
    "    if diff < 10:\n",
    "        break\n",
    "    P_prev = P\n",
    "print(\"----------------------------------------------------\")\n",
    "error = clustering()\n",
    "print('Total iteration to coverage: {}'.format(iteration))\n",
    "print('Total error rate: {}'.format(error/600000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.931e+03 8.000e+00 2.340e+02 2.450e+02 3.320e+02 3.100e+01 5.330e+02\n",
      "  2.201e+03 1.300e+01 3.950e+02]\n",
      " [1.020e+02 5.904e+03 3.180e+02 0.000e+00 1.500e+01 0.000e+00 3.600e+01\n",
      "  1.000e+00 2.000e+00 3.640e+02]\n",
      " [1.294e+03 9.530e+02 1.590e+02 9.000e+01 1.090e+02 1.200e+01 1.310e+02\n",
      "  1.867e+03 1.740e+02 1.169e+03]\n",
      " [9.210e+02 8.250e+02 4.980e+02 1.300e+01 3.400e+01 0.000e+00 9.000e+00\n",
      "  2.640e+02 2.677e+03 8.900e+02]\n",
      " [6.500e+01 7.300e+01 3.232e+03 9.120e+02 1.060e+02 6.000e+00 1.418e+03\n",
      "  1.400e+01 0.000e+00 1.600e+01]\n",
      " [1.345e+03 6.700e+01 1.433e+03 2.600e+01 9.800e+01 1.000e+00 9.370e+02\n",
      "  5.300e+01 4.600e+02 1.001e+03]\n",
      " [4.760e+02 1.990e+02 1.800e+01 3.100e+01 4.028e+03 9.660e+02 1.400e+01\n",
      "  7.600e+01 0.000e+00 1.100e+02]\n",
      " [1.300e+01 1.570e+02 4.885e+03 2.240e+02 0.000e+00 0.000e+00 9.390e+02\n",
      "  4.000e+00 4.000e+00 3.900e+01]\n",
      " [1.883e+03 3.900e+02 1.281e+03 2.200e+01 2.300e+01 2.000e+00 3.810e+02\n",
      "  1.500e+02 6.270e+02 1.092e+03]\n",
      " [2.600e+01 4.800e+01 4.460e+03 6.620e+02 2.000e+00 0.000e+00 6.460e+02\n",
      "  1.500e+01 4.500e+01 4.500e+01]]\n",
      "labeled class 0:\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000001111100000000000\n",
      "0000000000111111110000000000\n",
      "0000000000111111111000000000\n",
      "0000000001111111111100000000\n",
      "0000000000100000011100000000\n",
      "0000000000000000011110000000\n",
      "0000000000000000001110000000\n",
      "0000000000000000001110000000\n",
      "0000000000000000001110000000\n",
      "0000000000000000011110000000\n",
      "0000000000000000011110000000\n",
      "0000000000000000011110000000\n",
      "0000000111000000000111000000\n",
      "0000000110000000001111000000\n",
      "0000001111000000001111000000\n",
      "0000001111000001111111000000\n",
      "0000001111111111111110000000\n",
      "0000000111111111111000000000\n",
      "0000000001111111100000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "\n",
      "labeled class 1:\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000100000000000\n",
      "0000000000000000110000000000\n",
      "0000000000000000110000000000\n",
      "0000000000000000110000000000\n",
      "0000000000000001110000000000\n",
      "0000000000000001110000000000\n",
      "0000000000000011100000000000\n",
      "0000000000000011100000000000\n",
      "0000000000000111100000000000\n",
      "0000000000000111000000000000\n",
      "0000000000000110000000000000\n",
      "0000000000000110000000000000\n",
      "0000000000001100000000000000\n",
      "0000000000001100000000000000\n",
      "0000000000001000000000000000\n",
      "0000000000011000000000000000\n",
      "0000000000111000000000000000\n",
      "0000000000111000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "\n",
      "labeled class 2:\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000001111100000000000\n",
      "0000000000111111100000000000\n",
      "0000000000111111000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000001000000000000\n",
      "0000000000000111100000000000\n",
      "0000000000000111100000000000\n",
      "0000000000000111000000000000\n",
      "0000000000000111000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000100000000\n",
      "0000000000011110011000000000\n",
      "0000000000011111110000000000\n",
      "0000000000000111000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "\n",
      "labeled class 3:\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000011111100000000000\n",
      "0000000001111111111000000000\n",
      "0000000011111111111000000000\n",
      "0000000011100000111000000000\n",
      "0000000000000000111000000000\n",
      "0000000000000001111000000000\n",
      "0000000000000011110000000000\n",
      "0000000000001111110000000000\n",
      "0000000000011111110000000000\n",
      "0000000000011111111000000000\n",
      "0000000000001111111100000000\n",
      "0000000000000000011100000000\n",
      "0000000000000000001100000000\n",
      "0000000000000000001110000000\n",
      "0000000000000000011100000000\n",
      "0000000000000000111100000000\n",
      "0000000011111111111100000000\n",
      "0000000011111111111000000000\n",
      "0000000001111111100000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "\n",
      "labeled class 4:\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000100000111000000\n",
      "0000000000011100000111000000\n",
      "0000000000111000001110000000\n",
      "0000000001110000001100000000\n",
      "0000000001100000011000000000\n",
      "0000000011100000010000000000\n",
      "0000000011000000110000000000\n",
      "0000000011000011100000000000\n",
      "0000000000001111100000000000\n",
      "0000000000000111000000000000\n",
      "0000000000000110000000000000\n",
      "0000000000001110000000000000\n",
      "0000000000001100000000000000\n",
      "0000000000011000000000000000\n",
      "0000000000111000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "\n",
      "labeled class 5:\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000010000000000000000\n",
      "0000000001110000000000000000\n",
      "0000000001110000000000000000\n",
      "0000000011100000000000000000\n",
      "0000000011100000000000000000\n",
      "0000000011000000000000000000\n",
      "0000000111000000000000000000\n",
      "0000000111000000000000000000\n",
      "0000000110000000000011000000\n",
      "0000000110000000011111100000\n",
      "0000000110000000111111110000\n",
      "0000000110000001110001110000\n",
      "0000000110000011100001110000\n",
      "0000000111000011000001110000\n",
      "0000000111100011000001110000\n",
      "0000000011111111111111100000\n",
      "0000000001111111111111000000\n",
      "0000000000111111111100000000\n",
      "0000000000000111110000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "\n",
      "labeled class 6:\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000001100000000000\n",
      "0000000000000011100000000000\n",
      "0000000000000111000000000000\n",
      "0000000000001111000000000000\n",
      "0000000000001110000000000000\n",
      "0000000000011100000000000000\n",
      "0000000000011100000000000000\n",
      "0000000000111000000000000000\n",
      "0000000000111000000000000000\n",
      "0000000001110000011100000000\n",
      "0000000001110001111110000000\n",
      "0000000001110011111110000000\n",
      "0000000001110010001110000000\n",
      "0000000011110000001110000000\n",
      "0000000001111100111100000000\n",
      "0000000001111111111000000000\n",
      "0000000001111111110000000000\n",
      "0000000000111111100000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "\n",
      "labeled class 7:\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000111110000000000\n",
      "0000000000011111111000000000\n",
      "0000000000111000011000000000\n",
      "0000000000110000001000000000\n",
      "0000000000100000001000000000\n",
      "0000000000000000011000000000\n",
      "0000000000000000110000000000\n",
      "0000000000000001110000000000\n",
      "0000000000000011110000000000\n",
      "0000000000000011110000000000\n",
      "0000000000000001100000000000\n",
      "0000000000000001100000000000\n",
      "0000000000000001100000000000\n",
      "0000000000000011000000000000\n",
      "0000000000000011000000000000\n",
      "0000000000000010000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "\n",
      "labeled class 8:\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000001111110000000\n",
      "0000000000000011111110000000\n",
      "0000000000000111111111000000\n",
      "0000000000000111000011000000\n",
      "0000000000001110000010000000\n",
      "0000000000001100000000000000\n",
      "0000000000001100000000000000\n",
      "0000000000011100000000000000\n",
      "0000000000011111110000000000\n",
      "0000000000011111110000000000\n",
      "0000000000001111100000000000\n",
      "0000000000001110000000000000\n",
      "0000000000000000110000000000\n",
      "0000000000000001110000000000\n",
      "0000000111100011110000000000\n",
      "0000001111111111100000000000\n",
      "0000000111111111000000000000\n",
      "0000000111111100000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "\n",
      "labeled class 9:\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000001000000000000000000\n",
      "0000000111100000000000000000\n",
      "0000001111000000011100000000\n",
      "0000001110000000011110000000\n",
      "0000011110000000011110000000\n",
      "0000011110000000001110000000\n",
      "0000011110000000011110000000\n",
      "0000011110000000011110000000\n",
      "0000001111000000011110000000\n",
      "0000001111111100011110000000\n",
      "0000000111110000011110000000\n",
      "0000000000000000011110000000\n",
      "0000000000000000001110000000\n",
      "0000000000000000001110000000\n",
      "0000000000000000001110000000\n",
      "0000000000000000000100000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "\n",
      "\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "                Predict number 0 Predict not number 0\n",
      "Is number 0           2201.0                3722.0\n",
      "Isn't number 0       2444.0                51633.0\n",
      "\n",
      "Sensitivity (Successfully predict cluster 1): 0.37160\n",
      "Specificity (Successfully predict cluster 2): 0.95481\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predict number 1 Predict not number 1\n",
      "Is number 1           5904.0                838.0\n",
      "Isn't number 1       2720.0                50538.0\n",
      "\n",
      "Sensitivity (Successfully predict cluster 1): 0.87570\n",
      "Specificity (Successfully predict cluster 2): 0.94893\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predict number 2 Predict not number 2\n",
      "Is number 2           1169.0                4789.0\n",
      "Isn't number 2       3952.0                50090.0\n",
      "\n",
      "Sensitivity (Successfully predict cluster 1): 0.19621\n",
      "Specificity (Successfully predict cluster 2): 0.92687\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predict number 3 Predict not number 3\n",
      "Is number 3           2677.0                3454.0\n",
      "Isn't number 3       1325.0                52544.0\n",
      "\n",
      "Sensitivity (Successfully predict cluster 1): 0.43663\n",
      "Specificity (Successfully predict cluster 2): 0.97540\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predict number 4 Predict not number 4\n",
      "Is number 4           1418.0                4424.0\n",
      "Isn't number 4       3626.0                50532.0\n",
      "\n",
      "Sensitivity (Successfully predict cluster 1): 0.24273\n",
      "Specificity (Successfully predict cluster 2): 0.93305\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predict number 5 Predict not number 5\n",
      "Is number 5           1.0                5420.0\n",
      "Isn't number 5       1017.0                53562.0\n",
      "\n",
      "Sensitivity (Successfully predict cluster 1): 0.00018\n",
      "Specificity (Successfully predict cluster 2): 0.98137\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predict number 6 Predict not number 6\n",
      "Is number 6           4028.0                1890.0\n",
      "Isn't number 6       719.0                53363.0\n",
      "\n",
      "Sensitivity (Successfully predict cluster 1): 0.68064\n",
      "Specificity (Successfully predict cluster 2): 0.98671\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predict number 7 Predict not number 7\n",
      "Is number 7           4885.0                1380.0\n",
      "Isn't number 7       11633.0                42102.0\n",
      "\n",
      "Sensitivity (Successfully predict cluster 1): 0.77973\n",
      "Specificity (Successfully predict cluster 2): 0.78351\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predict number 8 Predict not number 8\n",
      "Is number 8           1883.0                3968.0\n",
      "Isn't number 8       6173.0                47976.0\n",
      "\n",
      "Sensitivity (Successfully predict cluster 1): 0.32183\n",
      "Specificity (Successfully predict cluster 2): 0.88600\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predict number 9 Predict not number 9\n",
      "Is number 9           662.0                5287.0\n",
      "Isn't number 9       1563.0                52488.0\n",
      "\n",
      "Sensitivity (Successfully predict cluster 1): 0.11128\n",
      "Specificity (Successfully predict cluster 2): 0.97108\n",
      "\n",
      "----------------------------------------------------\n",
      "Total iteration to coverage: 2\n",
      "Total error rate: 0.11724\n"
     ]
    }
   ],
   "source": [
    "error = clustering()\n",
    "print('Total iteration to coverage: {}'.format(iteration))\n",
    "print('Total error rate: {}'.format(error/600000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(abs(np.array([[-1,3,-2],[2,3,-6]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
